{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator and Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://252f7d7c2f69:4040\n",
       "SparkContext available as 'sc' (version = 2.4.2, master = local[*], app id = local-1559548724393)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Set log level to ERROR (less verbose)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.sql.Row\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0| [0.0,1.1,0.1]|\n",
      "|  0.0|[2.0,1.0,-1.0]|\n",
      "|  0.0| [2.0,1.3,1.0]|\n",
      "|  1.0|[0.0,1.2,-0.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Prepare training data from a list of (label, features) tuples.\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0|[-1.0,1.5,1.3]|\n",
      "|  0.0|[3.0,2.0,-0.1]|\n",
      "|  1.0|[0.0,2.2,-1.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Prepare test data.\n",
    "val test = spark.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_f463ed4a4d9d\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a LogisticRegression instance. This instance is an Estimator.\n",
    "val lr = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      " aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Print out the parameters, documentation, and any default values.\n",
    "println(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: lr.type = logreg_f463ed4a4d9d\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We may set parameters using setter methods.\n",
    "lr.setMaxIter(10)\n",
    "  .setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMap: org.apache.spark.ml.param.ParamMap =\n",
       "{\n",
       "\tlogreg_f463ed4a4d9d-maxIter: 30,\n",
       "\tlogreg_f463ed4a4d9d-regParam: 0.1,\n",
       "\tlogreg_f463ed4a4d9d-threshold: 0.55\n",
       "}\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We may alternatively specify parameters using a ParamMap,\n",
    "// which supports several methods for specifying parameters.\n",
    "val paramMap = ParamMap(lr.maxIter -> 20)\n",
    "  .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.\n",
    "  .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_f463ed4a4d9d, numClasses = 2, numFeatures = 3\n",
       "model2: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_f463ed4a4d9d, numClasses = 2, numFeatures = 3\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "val model1 = lr.fit(training)\n",
    "\n",
    "// paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "val model2 = lr.fit(training, paramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: {\n",
      "\tlogreg_f463ed4a4d9d-aggregationDepth: 2,\n",
      "\tlogreg_f463ed4a4d9d-elasticNetParam: 0.0,\n",
      "\tlogreg_f463ed4a4d9d-family: auto,\n",
      "\tlogreg_f463ed4a4d9d-featuresCol: features,\n",
      "\tlogreg_f463ed4a4d9d-fitIntercept: true,\n",
      "\tlogreg_f463ed4a4d9d-labelCol: label,\n",
      "\tlogreg_f463ed4a4d9d-maxIter: 10,\n",
      "\tlogreg_f463ed4a4d9d-predictionCol: prediction,\n",
      "\tlogreg_f463ed4a4d9d-probabilityCol: probability,\n",
      "\tlogreg_f463ed4a4d9d-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_f463ed4a4d9d-regParam: 0.01,\n",
      "\tlogreg_f463ed4a4d9d-standardization: true,\n",
      "\tlogreg_f463ed4a4d9d-threshold: 0.5,\n",
      "\tlogreg_f463ed4a4d9d-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "// LogisticRegression instance.\n",
    "println(s\"Model 1 was fit using parameters: ${model1.parent.extractParamMap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 was fit using parameters: {\n",
      "\tlogreg_f463ed4a4d9d-aggregationDepth: 2,\n",
      "\tlogreg_f463ed4a4d9d-elasticNetParam: 0.0,\n",
      "\tlogreg_f463ed4a4d9d-family: auto,\n",
      "\tlogreg_f463ed4a4d9d-featuresCol: features,\n",
      "\tlogreg_f463ed4a4d9d-fitIntercept: true,\n",
      "\tlogreg_f463ed4a4d9d-labelCol: label,\n",
      "\tlogreg_f463ed4a4d9d-maxIter: 30,\n",
      "\tlogreg_f463ed4a4d9d-predictionCol: prediction,\n",
      "\tlogreg_f463ed4a4d9d-probabilityCol: probability,\n",
      "\tlogreg_f463ed4a4d9d-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_f463ed4a4d9d-regParam: 0.1,\n",
      "\tlogreg_f463ed4a4d9d-standardization: true,\n",
      "\tlogreg_f463ed4a4d9d-threshold: 0.55,\n",
      "\tlogreg_f463ed4a4d9d-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Now learn a new model using the paramMapCombined parameters.\n",
    "println(s\"Model 2 was fit using parameters: ${model2.parent.extractParamMap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-6.5872014439355...|[0.00137599470692...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[3.98018281942565...|[0.98166040093741...|       0.0|\n",
      "|  1.0|[0.0,2.2,-1.5]|[-6.3765177028604...|[0.00169814755783...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n",
      "Model 2\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-2.8046569418746...|[0.05707304171034...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[2.49587635664205...|[0.92385223117041...|       0.0|\n",
      "|  1.0|[0.0,2.2,-1.5]|[-2.0935249027913...|[0.10972776114779...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Make predictions on test data using the Transformer.transform() method.\n",
    "// LogisticRegression.transform will only use the 'features' column.\n",
    "println(\"Model 1\")\n",
    "model1.transform(test)\n",
    "  .show()\n",
    "\n",
    "println(\"Model 2\")\n",
    "model2.transform(test)\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
